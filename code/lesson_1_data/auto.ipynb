{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y = 2*torch.dot(x,x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(2.0)\n",
    "a.requires_grad_(True),b.requires_grad_(True)\n",
    "z = a**2+4*b\n",
    "z\n",
    "\n",
    "\n",
    "z.backward()\n",
    "a.grad\n",
    "\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度清零 x.grad.zero_()\n",
    "在 PyTorch 中，`.grad.zero_()` 方法用于将张量的梯度归零。这通常用在神经网络的训练循环中，在每次进行梯度计算之前清零累积的梯度。具体到 `x.grad.zero_()`，这个方法将 `x` 的梯度重置为零。\n",
    "\n",
    "神经网络训练中为什么需要归零梯度：\n",
    "\n",
    "1. **累积梯度**：PyTorch 默认会累积梯度，这意味着每次调用 `.backward()` 时，梯度不是被覆盖，而是累加到已存在的梯度上。这在某些情况（如循环神经网络）是有用的，但在大多数情况下我们希望每次迭代只处理当前批次的梯度。\n",
    "\n",
    "2. **清除历史梯度**：如果不清零，梯度会持续累积，导致每一次迭代的梯度都是基于之前所有迭代的梯度之和，这会妨碍正确的梯度计算和网络的有效训练。\n",
    "\n",
    "3. **每次迭代独立**：通过在每个训练迭代开始前归零梯度，确保每个迭代步骤的梯度计算是独立的，只反映当前批次数据的损失梯度。\n",
    "\n",
    "例子：\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in dataset:\n",
    "        optimizer.zero_grad()  # 用优化器清零所有模型参数的梯度\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()  # 计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "```\n",
    "\n",
    "在这个例子中，`optimizer.zero_grad()` 调用确保了每次进入一个新的数据批次时，之前批次的梯度不会影响当前批次的梯度计算。在一些情况下，你可能会看到类似 `x.grad.zero_()` 的代码，这是直接对特定张量的梯度进行归零，而 `optimizer.zero_grad()` 是更常用的做法，它会清零优化器中所有参数的梯度。\n",
    "\n",
    "PyTorch 之所以设计为默认累积梯度，主要是基于以下几个原因：\n",
    "\n",
    "1. **灵活性**：累积梯度提供了更大的灵活性。在某些情况下，如训练大型网络时内存不足，或者在特定的模型（如循环神经网络）中，可能需要跨多个批次累积梯度。这样可以在不增加内存压力的情况下有效地处理大型数据集或复杂模型。\n",
    "\n",
    "2. **支持更复杂的优化策略**：在某些高级优化策略中，可能需要在更新模型参数之前考虑多个前向传播的结果。累积梯度的特性允许开发者实现这些更复杂的策略。\n",
    "\n",
    "3. **效率和性能**：在某些情况下，比如分布式训练，累积梯度可以减少通信次数，从而提高效率。在每个节点上累积更多的梯度数据，然后进行一次更大的参数更新，可以减少网络通信带来的开销。\n",
    "\n",
    "4. **模型架构的灵活性**：某些模型架构，特别是那些有多个输出或多个任务（如多任务学习）的模型，可能需要根据不同的损失函数累积梯度。这种设计使得在单次传递中对多个任务的梯度进行累积成为可能。\n",
    "\n",
    "总的来说，PyTorch 选择累积梯度是为了提供更大的灵活性和适应更广泛的使用场景，尤其是在复杂的训练情况和高级优化策略中。但这也意味着在标准的训练循环中，开发者需要显式地在每个迭代步骤开始前清除梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果x为向量\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "\n",
    "y = x*x\n",
    "\n",
    "y.sum().backward()\n",
    "#y.backward(torch.ones(len(x)))\n",
    "x.grad \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对于非标量y，使用sum求其梯度\n",
    "假设 $\\mathbf{x}$ 是一个向量，其元素为 $x_1, x_2, ..., x_n$。接着定义 $\\mathbf{y}$ 作为 $\\mathbf{x}$ 的元素平方的向量，即 $y_i = x_i^2$ 对于所有 $i$。然后计算 $\\mathbf{y}$ 的元素和：\n",
    "$$z = \\sum_{i=1}^{n} y_i$$\n",
    "我们关心的是 $z$ 相对于 $\\mathbf{x}$ 的梯度。首先，考虑 $z$ 相对于 $\\mathbf{y}$ 的梯度。由于 $z$ 是 $\\mathbf{y}$ 的元素和，所以 $z$ 相对于每个 $y_i$ 的偏导数是 1：\n",
    "$$\\frac{\\partial z}{\\partial y_i} = 1$$\n",
    "接下来，根据链式法则，$z$ 相对于 $x_i$ 的梯度可以表示为：\n",
    "$$\\frac{\\partial z}{\\partial x_i} = \\frac{\\partial z}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial x_i}$$\n",
    "由于 $y_i = x_i^2$，我们有：\n",
    "$$\\frac{\\partial y_i}{\\partial x_i} = 2x_i$$\n",
    "因此：\n",
    "$$\\frac{\\partial z}{\\partial x_i} = 1 \\cdot 2x_i = 2x_i$$\n",
    "当执行 `y.sum().backward()` 操作时，PyTorch 会自动进行这些计算，结果就是 $\\mathbf{x}$ 的梯度（即 $\\mathbf{x}$.grad）将包含 $2 \\mathbf{x}$ 的值。这意味着 $\\mathbf{x}$.grad 中的每个元素都是对应的 $x_i$ 的两倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4.,  7., 12.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "y = x*x+4\n",
    "u = y.detach()\n",
    "z = u*x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
